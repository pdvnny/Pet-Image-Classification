{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "458a64ce",
   "metadata": {},
   "source": [
    "### Parker Dunn\n",
    "\n",
    "## EK 381 - HW 10 - Optional Competition\n",
    "\n",
    "### DUE: December 3rd, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5fb402",
   "metadata": {},
   "source": [
    "### PLAN\n",
    "\n",
    "1. Generate a simple algorithm on my own - use reduced data from **PCA** along with the concept of **nearest neighbor**\n",
    "2. (b) (*maybe*) try to use a series of classifiers with weighted significance\n",
    "    \n",
    "    a. Not sure how I would weight them  \n",
    "    b. __use multiple PCA & nearest neighbor levels__!!  \n",
    "        (1) pull out the misclassified data after each attempt at classification\n",
    "        (2) try to classify the missed data - weight this stage more heaviliy\n",
    "        (3) repeat  \n",
    "    c. (*probably*) maybe use *perceptron* or *linear discriminant analysis* instead of nearest neighbors to start  \n",
    "    d. ALSO, **figure out how to determine what percentage of variance is explained by each principle component**\n",
    "    \n",
    "    \n",
    "3. Try to use a deep learning algorithm  \n",
    "    a. Gotta figure out if there is an algorithm that works well with data format that I have available\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe874b",
   "metadata": {},
   "source": [
    "## List of function that will be helpful:\n",
    "\n",
    "* Error_rate\n",
    "* split_data\n",
    "* PCA function\n",
    "\n",
    "\n",
    "* (NEW) function to calculate \"distance\" between images\n",
    "* `average_pet()` ... probably\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf5b638",
   "metadata": {},
   "source": [
    "# PART I - SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81034467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Modules\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt \n",
    "from skimage import io\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# MY IMPORTS\n",
    "import math\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6251ca9c",
   "metadata": {},
   "source": [
    "### `read_data()` - copied from assignment to get the data into this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "986eee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function reads in all n images in catsfolder/ and dogsfolder/. \n",
    "#Each 64 x 64 image is reshaped into a length-4096 row vector. \n",
    "#These row vectors are stacked on top of one another to get a data matrix\n",
    "#X of size n x 4096. We also generate a -1 label if the row vector corresponds\n",
    "#to a cat image and a +1 label if the row vector corresponds to a dog image\n",
    "#and stack these on top of one another to get a label vector y of length n.\n",
    "\n",
    "def read_data():\n",
    "    \n",
    "    # get image filenames\n",
    "    cat_locs = glob.glob('petdataset/catsfolder/*.jpg')\n",
    "    dog_locs = glob.glob('petdataset/dogsfolder/*.jpg')\n",
    "    num_cats = len(cat_locs)\n",
    "    num_dogs = len(dog_locs)\n",
    "    \n",
    "    # initialize empty arrays\n",
    "    X_cats = np.zeros((num_cats,64*64))\n",
    "    X_dogs = np.zeros((num_dogs,64*64))\n",
    "    y_cats = np.zeros((num_cats,1))\n",
    "    y_dogs = np.zeros((num_dogs,1))\n",
    "              \n",
    "    #Load data, reshape into a 1D vector and set labels\n",
    "    \n",
    "    keep_track = 0\n",
    "\n",
    "    for i in range(num_cats):\n",
    "        img = cat_locs[i]\n",
    "        im = io.imread(img)\n",
    "        im = im.reshape(64*64)\n",
    "        X_cats[i,:] = im\n",
    "        y_cats[i] = -1.0\n",
    "        keep_track += 1\n",
    "\n",
    "    for i in range(num_dogs):\n",
    "        img = dog_locs[i]\n",
    "        im = io.imread(img)\n",
    "        im = im.reshape(64*64)\n",
    "        X_dogs[i,:] = im\n",
    "        y_dogs[i] = 1.0\n",
    "        keep_track += 1\n",
    "    \n",
    "    # combine both datasets\n",
    "    X = np.append(X_cats,X_dogs,0)\n",
    "    y = np.append(y_cats,y_dogs)\n",
    "    \n",
    "    return X, y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34e298e",
   "metadata": {},
   "source": [
    "### `split_data()` - copied from assignment to create testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb74415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes in a data matrix X, label vector y, and \n",
    "#the desired percentage testpercent. It randomly assigns  \n",
    "#testpercent of the rows of X and y to Xtest and ytest. \n",
    "#The remaining data is assigned to Xtrain and ytrain.\n",
    "\n",
    "def split_data(X,y,testpercent):\n",
    "        \n",
    "    [n, d] = X.shape\n",
    "    \n",
    "    ntest = int(round(n*(float(testpercent)/100)))\n",
    "    ntrain = int(round(n - ntest))\n",
    "        \n",
    "    Xtrain = np.zeros((ntrain,d))\n",
    "    Xtest = np.zeros((ntest,d))\n",
    "    ytrain = np.zeros((ntrain,1))\n",
    "    ytest = np.zeros((ntest,1))   \n",
    "        \n",
    "    Data = np.column_stack((X,y))\n",
    "    Data = np.random.permutation(Data)\n",
    "    \n",
    "    for i in range(ntest):\n",
    "        Xtest[i,:] = Data[i,0:d]\n",
    "        ytest[i] = Data[i,d]\n",
    "        \n",
    "    for i in range(ntrain):\n",
    "        Xtrain[i,:] = Data[i+ntest,0:d]\n",
    "        ytrain[i] = Data[i+ntest,d]\n",
    "        \n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e13814",
   "metadata": {},
   "source": [
    "#### Using copied functions to prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c40684c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from the image files\n",
    "X, y = read_data()\n",
    "\n",
    "# Splitting data into data for training and testing\n",
    "Xtrain, ytrain, Xtest, ytest = split_data(X,y,10)\n",
    "# 10% of the data reserved for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3b40c4",
   "metadata": {},
   "source": [
    "### `error_rate()` - copied from assignment to eventually use for testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c1876bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes in a vector of true labels ytrue\n",
    "#and a vector of guessed labels yguess and reports back\n",
    "#the error rate of the guesses as a percentage 0% to 100%.\n",
    "\n",
    "def error_rate(ytrue, yguess):\n",
    "    # compare your predictions with the correct labels to determine how many of your predictions were correct.\n",
    "    total = len(ytrue)\n",
    "    wrong = np.sum(np.not_equal(ytrue,yguess))\n",
    "    error = 100*wrong/total\n",
    "    # divide the number of correct predictions by the number of total samples to determine your classification accuracy.\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "395120de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code implements the PCA exactly as in MATLAB so as to be consistent.\n",
    "#It takes in an n x d data matrix X and returns a d x d orthonormal matrix pcaX. \n",
    "#Each column of pcaX contains a basis vector, sorted by decreasing variance.\n",
    "\n",
    "def pca(X):\n",
    "    covX = np.cov(X,rowvar=False)\n",
    "    [Lambda,Vtranspose] = np.linalg.eig(covX)\n",
    "    neworder = np.argsort(-abs(Lambda))\n",
    "    pcaX = Vtranspose[:,neworder]\n",
    "    pcaX = pcaX.real\n",
    "\n",
    "    return pcaX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86ab0f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes in a training data matrix Xtrain, training\n",
    "#label vector ytrain and uses them to the decision rule corresponding\n",
    "#to a very simple one-layer neural network: the perceptron. \n",
    "#It also takes in a data matrix Xrun and produces a vector of label\n",
    "#guesses yguess, corresponding to the sign of the linear prediction.\n",
    "\n",
    "def perceptron(Xtrain,ytrain,Xrun):\n",
    "    # Doing one-time calculation\n",
    "    inv_Xtrain = np.linalg.pinv(Xtrain.T @ Xtrain)  # dimensions: (4096 x rows) * (rows * 4096) to work with the other vector ... it hasto be done this way\n",
    "    xT_by_ytrain = Xtrain.T @ ytrain          # dimensions: (rows x 4096) * (rows * 1) -- > (4096 x rows) * (rows * 1) !!! only way this works\n",
    "    multiply_vec = inv_Xtrain @ xT_by_ytrain\n",
    "    \n",
    "    # creating yguess\n",
    "    yguess = np.zeros((Xrun.shape[0], 1))\n",
    "    \n",
    "    # Extracting and manipulating each row of Xrun\n",
    "    for i in range(Xrun.shape[0]):\n",
    "        X_vec = Xrun[i,:]\n",
    "        X_arr = np.array([X_vec])\n",
    "#         if (i == 0):\n",
    "#             print(X_arr.shape)  # need this to be a column vector!\n",
    "#             print(multiply_vec.shape)\n",
    "        if ((X_arr @ multiply_vec) >= 0):\n",
    "            yguess[i,0] = 1\n",
    "        else:\n",
    "            yguess[i,0] = -1\n",
    "            \n",
    "    return yguess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d78a5a",
   "metadata": {},
   "source": [
    "# Next Up: \"THE FUNCTION\"\n",
    "### This next code block contains the `pet_classifier()` function which will be trained using another function within this script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd7ab71",
   "metadata": {},
   "source": [
    "Trying to understand the design of a classifier algorithm better...\n",
    "\n",
    "#### What classifier needs to be able to do!\n",
    "* center X\n",
    "* Matrix multiplication of Xrun * Vk\n",
    "* Run the data through perceptron\n",
    "    * (!!) needs to have been trained OR needs Xtrain, ytrain\n",
    "* Process yguess (if it has multiple columns that need to be weighted and added)\n",
    "* Return yguess\n",
    "\n",
    "\n",
    "#### DEPLOYMENT QUESTIONS\n",
    "* What am I going to save for a trained model?\n",
    "* How am I going to \"re-train\" the model when somone else opens the program?\n",
    "\n",
    "\n",
    "#### TRAINING QUESTIONS\n",
    "* Setup to test a combination of weights every time. Do I need to use multiple weight predictions for the same data?\n",
    "\n",
    "\n",
    "* Do I want to use the weighting while training? (i.e. as I extract data, I can weight the data being used differently...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0488259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pet_classifier(Xtrain, ytrain, Xrun, k, weights, mu):\n",
    "    # Repeated steps: PCA/reduce_data() -> use classifier (e.g. k nearest neighbors) ->\n",
    "    #                -> determine which guesses are wrong -> extract incorrect guess rows from X \n",
    "    #                -> *REPEAT*\n",
    "    \n",
    "    ytest_guesses = np.zeros((Xrun.shape[0], len(weights)))\n",
    "    ytraining_guesses = np.zeros((Xtrain.shape[0], len(weights)))\n",
    "    # number of rows    ==  the number of measurements\n",
    "    # number of columns ==  the number of weights used\n",
    "    \n",
    "    # REDUCING THE DATA - PCA & data reduction\n",
    "    Xtrain_cent = Xtrain - mu        # center training data\n",
    "    Xrun_cent = Xrun - mu            # center testing data    \n",
    "    pcaXtrain = pca(Xtrain_cent)            # Run PCA\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        \n",
    "        wt_k = round(k * weights[i])\n",
    "        print(\"wt_k is {}\".format(wt_k))\n",
    "        Vk = pcaXtrain[0:wt_k,:].T       # Extract a portion of the eigenvectors\n",
    "\n",
    "        Xtrain_red = Xtrain_cent @ Vk\n",
    "        Xrun_red = Xrun_cent @ Vk\n",
    "\n",
    "    # CLASSIFY REDUCED DATA WITH PERCEPTRON\n",
    "        yguesstrain = perceptron(Xtrain_red, ytrain, Xtrain_red)    \n",
    "        yguesstest = perceptron(Xtrain_red, ytrain, Xrun_red)\n",
    "\n",
    "    # SAVE EACH OF THE RETURNED GUESS VECTORS\n",
    "        ytest_guesses[:,i] = yguesstest[:,0]\n",
    "        ytraining_guesses[:,i] = yguesstrain[:,0]\n",
    "    \n",
    "    # \"COMBINE\" COLUMNS OF \"ytest_guesses\" AND \"ytraining_guesses\"\n",
    "    ytest_result = np.zeros((ytest_guesses.shape[0], 1))\n",
    "    ytrain_result = np.zeros((ytraining_guesses.shape[0], 1))\n",
    "    \n",
    "    for row in range(ytraining_guesses.shape[0]):      # \"for each row\"\n",
    "        ytrain_result[row,0] = np.sign(np.sum(ytraining_guesses[row,:]))\n",
    "        \n",
    "    for row in range(ytest_guesses.shape[0]):\n",
    "        ytest_result[row,0] = np.sign(np.sum(ytest_guesses[row,:]))\n",
    "    \n",
    "    return ytrain_result, ytest_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39cc3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_testing = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "209de8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wt_k is 60\n",
      "wt_k is 120\n",
      "wt_k is 200\n",
      "{'ID183330': {'weights': [0.1, 0.25, 0.75], 'train_error': 7.5, 'test_error': 7.5}, 'ID184542': {'weights': [0.15, 0.3, 0.5], 'train_error': 6.611111111111111, 'test_error': 5.5}}\n"
     ]
    }
   ],
   "source": [
    "# Generate mu vector first\n",
    "mu = np.mean(Xtrain, axis=0)\n",
    "\n",
    "# PREPARING OTHER ARGUMENTS\n",
    "# Xtrain comes from above\n",
    "# ytrain comes from above\n",
    "# Xrun is Xtest from above\n",
    "k = 400\n",
    "weights = [0.15, 0.3, 0.5]   # right now this represents what number of the eigenvectors will be used\n",
    "# 0.1 -> 40\n",
    "# 0.25 -> 100\n",
    "# 0.75 -> 300\n",
    "\n",
    "yguesstrain, yguesstest = pet_classifier(Xtrain, ytrain, Xtest, k, weights, mu)\n",
    "training_error = error_rate(yguesstrain, ytrain)\n",
    "test_error = error_rate(yguesstest, ytest)\n",
    "\n",
    "# SAVING DATA FROM EACH RUN\n",
    "now = datetime.now()\n",
    "curtime = now.strftime(\"%H%M%S\")\n",
    "todays_testing[(\"ID\" + curtime)] = {\"weights\": weights, \"train_error\": training_error, \n",
    "                                    \"test_error\": test_error}\n",
    "print(todays_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7cdc91f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(todays_testing)\n",
    "# print(yguesstrain[0:20])\n",
    "# print(yguesstest[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "669b60fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE ALL OF THE DAYS TESTING/TRAINING\n",
    "\n",
    "now = datetime.now()\n",
    "today = date.today()\n",
    "\n",
    "day = today.strftime(\"%d%b%Y\")\n",
    "\n",
    "for key in todays_testing.keys():\n",
    "    with open(\"TrainingData/\" + day + \"_\" + key + \".json\", 'w') as file:\n",
    "        json.dump(todays_testing[key], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015031d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class Image_Classifier:\n",
    "    \n",
    "    def __init__(training = False):\n",
    "        self.k = 400\n",
    "        self.weights = [0.2, 0.35, 0.45]\n",
    "        self.training = training\n",
    "        calc_mu()\n",
    "        get_data()\n",
    "        if (not training):\n",
    "            getXtrain()\n",
    "            getytrain()\n",
    "    \n",
    "    def calc_mu(self):\n",
    "        if (self.training):\n",
    "            self.mu = np.mean(self.X, axis = 0)\n",
    "        else:\n",
    "            print(\"calc_mu: Not prepared yet\")\n",
    "    \n",
    "    def getXtrain(self, xtrain):\n",
    "        if (self.training):\n",
    "            self.Xtrain = xtrain\n",
    "        else:\n",
    "            print(\"calc_Xtrain: Not prepared yet\")\n",
    "    \n",
    "    def getytrain(self, ytrain):\n",
    "        if (self.training):\n",
    "            self.ytrain = ytrain\n",
    "        else:\n",
    "            print(\"calc_ytrain: Not prepared yet\")\n",
    "            \n",
    "#     def define_weights(self, weights):\n",
    "#         if (self.training):\n",
    "#             print(\"define_weights: will be loading weights from somewhere\")\n",
    "#         else:\n",
    "#             self.weights = weights\n",
    "    \n",
    "    def get_data(self, X):\n",
    "        if (not self.training):\n",
    "            self.X = X\n",
    "            \n",
    "    def perceptron(self)\n",
    "        if (self.training):\n",
    "            # use self.Xtrain\n",
    "        else:\n",
    "            # use self.X\n",
    "    \n",
    "    def pet_classifier(self, training = False):\n",
    "        # Repeated steps: PCA/reduce_data() -> use classifier (e.g. k nearest neighbors) ->\n",
    "        #                -> determine which guesses are wrong -> extract incorrect guess rows from X -> *REPEAT*\n",
    "\n",
    "        # PCA & data reduction\n",
    "        X_cent = self.X - self.mu        # center data\n",
    "        pcaXrun = pca(X_cent)            # Run PCA\n",
    "\n",
    "        yguess = np.zeros((self.X.shape[0], len(self.weights)))  \n",
    "        # number of rows    ==  the number of measurements\n",
    "        # number of columns ==  the number of weights used\n",
    "        \n",
    "        for wt in self.weights:\n",
    "            weighted_k = wt * k\n",
    "            Vk = pcaXrun[0:weighted_k,:].T     # Extract extract k features/rows and transpose \n",
    "                                               # to column eigenvectors\n",
    "            X_reduced = X_cent @ Vk\n",
    "\n",
    "            # NOW, use reduced data to make predictions\n",
    "            invX\n",
    "\n",
    "        return 0\n",
    "\n",
    "# def pet_classifier(Xrun, mu, weights = [.1, 0.25, 0.5], training=False):\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9a011b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
